# -*- coding: utf-8 -*-
import streamlit as st
import os
import nest_asyncio
from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding

# --- Import è·¯å¾‘ä¿®æ­£ ---
from llama_index.core.retrievers import VectorIndexRetriever, BaseRetriever
from llama_index.retrievers.bm25 import BM25Retriever
# ---------------------

from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core import PromptTemplate

# è§£æ±ºç•°æ­¥å•é¡Œ
nest_asyncio.apply()

# --- è¨­å®šé é¢ ---
st.set_page_config(page_title="å°ç©é›»å¹´å ± AI åŠ©æ‰‹", layout="wide")
st.title("ğŸ¤– TSMC å¹´å ± RAG åŠ©æ‰‹ (Gemini 2.5 + Hybrid Search)")

# --- å´é‚Šæ¬„è¨­å®š (é—œéµä¿®æ­£ï¼šåŠ å…¥ç´…ç¶ ç‡ˆ) ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸è®€å–é è¨­å€¼
    default_key = os.environ.get("GOOGLE_API_KEY", "")
    api_key = st.text_input("Google API Key", value=default_key, type="password")
    
    if api_key:
        os.environ["GOOGLE_API_KEY"] = api_key
    
    st.divider()
    st.info("è«‹ç¢ºä¿ ./storage è³‡æ–™å¤¾å·²ç¶“å»ºç«‹å®Œç•¢ (è«‹å…ˆåŸ·è¡Œéä¸€æ¬¡ main.py)")
    if st.button("ğŸ”„ é‡æ–°æ•´ç†"):
        st.rerun()

# --- [ç´…ç¶ ç‡ˆæª¢æŸ¥é»] ---
if not os.environ.get("GOOGLE_API_KEY"):
    st.warning("â¬…ï¸ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ Google API Key æ‰èƒ½é–‹å§‹å°è©±ï¼")
    st.stop()  # <--- é€™è£¡æœƒæš«åœç¨‹å¼åŸ·è¡Œï¼Œç›´åˆ°æœ‰ Key ç‚ºæ­¢
# --------------------

# --- æ ¸å¿ƒé¡åˆ¥å®šç¾© ---
class CustomHybridRetriever(BaseRetriever):
    def __init__(self, vector_retriever, bm25_retriever):
        self.vector_retriever = vector_retriever
        self.bm25_retriever = bm25_retriever
        super().__init__()

    def _retrieve(self, query_bundle):
        try:
            vec_nodes = self.vector_retriever.retrieve(query_bundle)
            bm25_nodes = self.bm25_retriever.retrieve(query_bundle)
            
            all_nodes = {}
            for node in vec_nodes: 
                all_nodes[node.node.node_id] = node
            for node in bm25_nodes:
                if node.node.node_id not in all_nodes: 
                    all_nodes[node.node.node_id] = node
            
            return list(all_nodes.values())[:20]
        except Exception as e:
            print(f"Retrieval Error: {e}")
            return []

# --- åˆå§‹åŒ–å¼•æ“ ---
@st.cache_resource
def load_rag_engine():
    persist_dir = "./storage"
    
    if not os.path.exists(persist_dir):
        return None
    
    try:
        # è¨­å®šæ¨¡å‹ (ç¢ºä¿é€™è£¡åŸ·è¡Œæ™‚å·²ç¶“æœ‰ API Key äº†)
        Settings.llm = Gemini(model="models/gemini-2.5-flash", api_key=os.environ["GOOGLE_API_KEY"])
        Settings.embed_model = GeminiEmbedding(model_name="models/text-embedding-004", api_key=os.environ["GOOGLE_API_KEY"])
        
        # è¼‰å…¥ç´¢å¼•
        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
        index = load_index_from_storage(storage_context)
        
        # å»ºç«‹æª¢ç´¢å™¨
        vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=10)
        bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=10)
        
        # çµ„åˆæ··åˆæª¢ç´¢å™¨
        retriever = CustomHybridRetriever(vector_retriever, bm25_retriever)
        
        # Prompt
        qa_prompt_str = (
            "ä»¥ä¸‹æ˜¯åƒè€ƒæ–‡ä»¶å…§å®¹ï¼š\n---------------------\n{context_str}\n---------------------\n"
            "è«‹åƒ…æ ¹æ“šä¸Šè¿°åƒè€ƒæ–‡ä»¶å…§å®¹ï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œ: {query_str}\n"
            "åš´æ ¼ç¦æ­¢ç·¨é€ æ–‡ä»¶ä¸­æœªæåŠçš„äººåã€æ•¸å­—æˆ–è·ç¨±ã€‚\n"
            "è«‹å‹™å¿…ä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ã€å›ç­”ï¼Œè‹¥æ˜¯è¡¨æ ¼æ•¸æ“šè«‹æ’ç‰ˆæ•´é½Šã€‚\n"
        )
        
        return RetrieverQueryEngine.from_args(
            retriever=retriever,
            text_qa_template=PromptTemplate(qa_prompt_str)
        )
    except Exception as e:
        st.error(f"å¼•æ“è¼‰å…¥å¤±æ•—: {e}")
        return None

# --- ä¸»é‚è¼¯ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è¼‰å…¥å¼•æ“
engine = load_rag_engine()

if engine is None:
    st.error("âŒ æ‰¾ä¸åˆ° ./storage ç´¢å¼•è³‡æ–™å¤¾ï¼è«‹å…ˆåŸ·è¡Œ `python main.py` ä¾†ç”Ÿæˆç´¢å¼•ã€‚")
else:
    # æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥
    if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ (ä¾‹å¦‚ï¼šè‘£äº‹æœƒæˆå“¡æœ‰å“ªäº›ï¼Ÿ)"):
        # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("AI æ­£åœ¨ç¿»é–±å¹´å ±ä¸­..."):
                try:
                    response = engine.query(prompt)
                    st.markdown(response.response)
                    
                    # é¡¯ç¤ºä¾†æº
                    with st.expander("ğŸ•µï¸ åƒè€ƒä¾†æºç‰‡æ®µ"):
                        for node in response.source_nodes:
                            score = f"{node.score:.2f}" if node.score is not None else "Hybrid"
                            st.caption(f"**[åˆ†æ•¸ {score}]**")
                            st.text(node.node.get_text()[:200] + "...")
                            st.divider()

                    # å„²å­˜ AI å›ç­”
                    st.session_state.messages.append({"role": "assistant", "content": response.response})
                
                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")